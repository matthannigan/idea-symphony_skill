# Question Generation Frameworks Research

**Date:** February 19, 2026  
**Purpose:** Map Idea Symphony personas against question generation methodologies to assess coverage, identify gaps, and evaluate the current Phase 2B persona set.

---

## Executive Summary

This research synthesizes established question generation methodologies from professional brainstorming and facilitated dialogue, mapping them against the 22 Idea Symphony personas to evaluate coverage and identify systematic gaps. The analysis reveals **critical deficiencies in the current Phase 2B persona set**, which employs exclusively analytical personas for question generation. This all-analytical composition systematically biases question territory toward evaluative, convergent, and analytical framing while underrepresenting generative, divergent, emotional, and systems-oriented question modes.

**Key findings:**

1. **Phase 2B analytical bias confirmed:** All five Phase 2B personas (Questioner, Analyst, Audience Advocate, Devil's Advocate, First Principles Thinker) operate in analytical or evaluative modes, creating systematic bias toward convergent, risk-focused, and assumption-probing questions while neglecting generative, imaginative, and affirmative framing.

2. **Critical gaps in question generation coverage:** The current roster lacks personas naturally suited to generate Appreciative Inquiry questions (positive/affirmative framing), Green Hat questions (creative possibilities), and provocative/lateral thinking questions in the question-generation phase.

3. **Mismatch between question-generation fitness and Phase 2B selection:** Several Phase 2B personas (Analyst, Questioner) are better suited to *answering* questions than *generating* them, while high-fitness question-generators (Provocateur, Visionary, Empath) are excluded from Phase 2B entirely.

4. **Downstream impact on brainstorming territory:** Because Phase 2 question clusters define the territory for all Phase 3 brainstorming, analytical-only question generation systematically constrains the creative, emotional, and transformative dimensions available to Phase 3 personas.

---

## 1. Question Generation Methodology Findings

### 1.1 Question Taxonomy

The research identified four primary taxonomic dimensions for classifying questions in facilitated brainstorming:

| Category | Sub-Category | Definition | Example | Cognitive Mode |
|:---------|:-------------|:-----------|:--------|:---------------|
| **Socratic Questioning** | Clarification | Ensure understanding and elicit more information | "Can you explain what you mean by that?" | Analytical |
| | Probing Assumptions | Challenge underlying beliefs and assumptions | "What are you taking for granted?" | Analytical |
| | Probing Reasons & Evidence | Encourage justification and evidence for claims | "What evidence supports your claim?" | Analytical |
| | Questioning Viewpoints | Encourage consideration of alternative viewpoints | "What is another way to look at it?" | Connective |
| | Probing Implications | Focus on potential outcomes and effects | "What are the consequences of that assumption?" | Analytical |
| | Meta-questions | Encourage reflection on the questioning process itself | "What was the point of this question?" | Analytical |
| **Question Design** | Divergent | Open-ended questions that encourage exploration and generation of multiple ideas | "What are all the possible ways we could address this issue?" | Generative |
| | Convergent | Questions designed to narrow down possibilities and arrive at a specific answer | "What is the primary cause of this problem?" | Analytical |
| **Question Framing** | Generative | Open-ended questions to explore, discover, and understand a problem space | "What challenges do you face in your role?" | Generative |
| | Evaluative | Questions used to assess how well a proposed solution meets user needs | "Does this feature help you complete your task more efficiently?" | Analytical |
| **Question Level/Depth** | Surface-Level | Inquiries that gather basic information, facts, and definitions | "What are the basic facts of the situation?" | Analytical |
| | Deep-Level | Questions that delve beneath the surface to explore underlying causes, motivations, and implications | "Why do you think that happened?" | Analytical/Generative |

**Key insight:** The taxonomy reveals that **most Socratic questioning categories are analytical in nature**, with only "questioning viewpoints" exhibiting connective characteristics. Divergent and generative question modes are explicitly distinguished from analytical modes, suggesting that analytical personas may be structurally unsuited to generate divergent/generative questions.

#### Detailed Analysis of Question Taxonomies

* **Socratic Questioning Categories:** Socratic questioning is a disciplined and thoughtful dialogue method used to explore complex ideas, uncover assumptions, and stimulate critical thinking. It is widely applied in teaching, counseling, and facilitated discussions to encourage deeper understanding and reflection. R.W. Paul's six types of Socratic questions provide a comprehensive framework for this methodology, as detailed in the taxonomy table above.

* **Divergent vs. Convergent Question Design:** Convergent and divergent thinking represent two fundamental modes of problem-solving, and the design of questions can either expand or narrow the solution space. Convergent questions are effective for tasks requiring precision and analysis, while divergent questions are crucial for stimulating creativity and exploring new possibilities. A balanced approach, moving between divergent and convergent questioning, is essential for effective creative problem-solving.

* **Generative vs. Evaluative Question Framing:** The framing of a question significantly influences the type and scope of responses. Generative framing broadens the response space, encouraging diverse ideas and insights, while evaluative framing narrows the focus to specific feedback on a proposed solution. The choice between generative and evaluative questions depends on the research phase, with generative questions dominating the discovery phase and evaluative questions becoming more prominent in the design and development phases.

* **Question Level/Depth (Surface vs. Deep):** Questions can be categorized by their level of depth, ranging from surface-level inquiries to deep-level questions. Surface-level questions are essential for establishing a common understanding, while deep-level questions challenge assumptions and provoke introspection. Effective facilitators balance these question types to manage the conversation's flow and depth, building rapport, ensuring clarity, and promoting engagement to achieve deeper insights [4].

### 1.2 Framework Question Design Principles

#### Appreciative Inquiry (4-D Cycle)

##### Summary

**Core principle:** Positive and affirmative framing that focuses on strengths, successes, and aspirations rather than problems and deficits.

**Question characteristics:**
- Stated in the affirmative, focusing on what is desired or what has worked well
- Forge personal connections by encouraging storytelling
- Assume a positive outlook
- Give broad definition to topics, allowing for rich narratives
- Invite narratives over abstract opinions
- Value what is already working
- Spark appreciative imagination by focusing on past/present successes
- Convey unconditional positive regard
- Evoke essential values, aspirations, and inspirations
- Grounded in real-world context
- Lead toward future possibilities

**Cognitive mode alignment:** Generative, Human-Centered, Forward-Looking

##### Principles of Question Design

Appreciative Inquiry (AI) is a strengths-based approach that focuses on identifying and amplifying what works well within a system, rather than dwelling on problems. The 4-D Cycle (Discover, Dream, Design, Destiny, or Inquire, Imagine, Innovate, Implement) guides this process. Question generation in AI is characterized by **positive and affirmative framing**, aiming to elicit stories of success, strengths, values, and aspirations. The core assumption is that questions and dialogue about strengths are inherently transformational [1].

AI questions are designed to be stated in the affirmative, focusing on what is desired or what has worked well. They forge personal connections by encouraging interviewees to share personal experiences and stories, assuming a positive outlook. These questions give a broad definition to the topic, allowing for rich storytelling rather than narrow, factual answers. They invite narratives over abstract opinions, valuing  what is already working and sparking appreciative imagination by focusing on past or present successes. A key element is conveying unconditional, positive regard to create a safe space for sharing. The questions aim to evoke essential values, aspirations, and inspirations, connecting to deeper motivations. They are grounded in real-world context by drawing on people’s life and work experience and are designed to lead towards future possibilities by suggesting action [1].

##### Examples of Questions for a Generic Brainstorming Topic (e.g., "Improving Team Collaboration")

The following table provides examples of questions for each phase of the AI 4-D Cycle, applied to the topic of improving team collaboration:

| 4-D Cycle Phase | Example Questions |
| :--- | :--- |
| **Discover (Inquire)** | - "Tell me about a time when our team collaboration was at its absolute best. What made it so successful and satisfying?"<br>- "What unique strengths do you bring to team collaboration, and how have they contributed to our past successes?"<br>- "When have you felt most alive and engaged while collaborating with colleagues? What was happening during that time?"<br>- "What are the core values that underpin our most effective collaborative efforts?" |
| **Dream (Imagine)** | - "Imagine our team collaboration three years from now, operating at its highest potential. What does it look like, sound like, and feel like?"<br>- "If a magic wand could instantly transform our team collaboration into its ideal state, what would be the most significant changes you would see?"<br>- "What bold aspirations do we have for our collaborative future that would truly set us apart?" |
| **Design (Innovate)** | - "What provocative propositions can we create that would ensure our team consistently experiences the peak collaboration moments we've described?"<br>- "How might we design our processes and interactions to amplify our collective strengths and foster even greater synergy?"<br>- "What specific elements of our 'dream' collaboration can we commit to building into our daily work?" |
| **Destiny (Implement)** | - "What immediate actions can each of us take to bring our provocative propositions for enhanced collaboration to life?"<br>- "How will we celebrate our successes and continuously learn as we implement these new collaborative practices?"<br>- "Who else needs to be involved, and what support do we need to sustain this elevated level of team collaboration?" |

#### Creative Problem Solving (CPS)

**Core principle:** Deliberate distinction between problem-finding questions and solution-finding questions; balancing divergent thinking (generating many ideas) and convergent thinking (evaluating and refining ideas).

**Question characteristics:**
- Open-ended to encourage wide range of responses
- Avoid premature judgment by separating idea generation from evaluation
- Focus on desired outcome, addressing root problems rather than symptoms
- Phrased positively using positive action verbs

**Cognitive mode alignment:** Analytical (fact-finding, problem-finding), Generative (solution-finding), Pragmatic (acceptance-finding)

##### Principles of Question Design

Creative Problem Solving (CPS) is a structured methodology for solving problems or finding opportunities that goes beyond conventional thinking. It emphasizes balancing **divergent thinking** (generating many ideas) and **convergent thinking** (evaluating and refining ideas). A key aspect of CPS question generation is the deliberate distinction between **problem-finding** and **solution-finding** questions, as well as other stages of the process. Questions in CPS are designed to explore the vision, gather data, and formulate challenges by sharpening awareness and creating open-ended questions that invite solutions. They are open-ended to encourage a wide range of responses and possibilities, and they avoid premature judgment by separating idea generation from evaluation. The questions focus on the desired outcome, addressing root problems rather than symptoms, and are phrased positively, using positive action verbs to encourage creativity [2].

##### Examples of Questions for a Generic Brainstorming Topic (e.g., "Improving Team Collaboration")

The following table illustrates question types within the CPS framework for improving team collaboration:

| CPS Stage | Question Type | Example Questions |
| :--- | :--- | :--- |
| **Explore the Vision** | Objective Finding | - "What is our ideal vision for team collaboration?"<br>- "What challenges are we currently facing in achieving optimal team collaboration?"<br>- "Wouldn't it be great if our team collaboration was...?" |
| **Gather Data** | Fact-Finding | - "What data do we have about our current team collaboration processes and outcomes?"<br>- "What are the specific instances where our team collaboration has been effective or ineffective?"<br>- "Who are the key stakeholders involved in team collaboration, and what are their perspectives?" |
| **Formulate Challenges** | Problem-Finding | - "How might we better understand the root causes of our collaboration challenges?"<br>- "In what ways can we reframe our current collaboration issues as opportunities for improvement?"<br>- "What are all the possible ways we could define the problem of 'suboptimal team collaboration'?" |
| **Ideate** | Solution-Finding | - "What are all the possible solutions to enhance our team collaboration?"<br>- "How might we encourage more open communication within the team?"<br>- "What innovative approaches could we take to foster a stronger sense of shared purpose?"<br>- "What if we completely reimagined our team meetings to be more engaging and productive?" |
| **Develop** | Solution-Finding | - "What criteria will we use to evaluate the potential solutions for improving team collaboration?"<br>- "Which of these solutions are most feasible and impactful for our team?"<br>- "How can we refine our chosen solutions to maximize their effectiveness?" |
| **Implement** | Acceptance-Finding | - "What steps are needed to put our chosen collaboration solutions into practice?"<br>- "Who will be responsible for implementing each aspect of the solution?"<br>- "How will we measure the success of our new collaboration strategies?" |

#### Design Thinking: How Might We (HMW)

**Core principle:** Bridge insights from user research to potential solutions through well-scoped, open-ended questions.

**Question characteristics:**
- Based on problems or insights, directly addressing identified user needs or pain points
- Avoid suggesting solutions, remaining open-ended
- Broad enough for creativity but narrow enough to be actionable
- Concentrate on desired outcome, addressing root problem
- Phrased positively

**Cognitive mode alignment:** Human-Centered, Generative, Pragmatic

##### Principles of Question Design

Design Thinking is a human-centered approach to innovation that focuses on understanding user needs and developing creative solutions. The 'How Might We' (HMW) question is a crucial tool in the ideation phase, bridging insights from user research to potential solutions. HMW questions are designed to be based on problems or insights, directly addressing identified user needs or pain points. They avoid suggesting solutions, remaining open-ended to encourage a wide range of ideas. These questions are broad enough for creativity but narrow enough to be actionable, striking a balance between inspiring diverse ideas and maintaining focus. They concentrate on the desired outcome, addressing the root problem and what needs to be achieved, and are phrased positively, using positive language to foster an optimistic and generative mindset [3].

##### Examples of Questions for a Generic Brainstorming Topic (e.g., "Improving Team Collaboration")

The following table provides examples of 'How Might We' questions for improving team collaboration, derived from common problems or insights:

| Problem/Insight | 'How Might We' Question Examples |
| :--- | :--- |
| Team members often feel their ideas aren't heard in meetings. | - "How might we create an environment where all team members feel empowered to share their ideas?"<br>- "How might we ensure equitable participation and voice in team discussions?" |
| Lack of clear communication leads to misunderstandings and duplicated effort. | - "How might we improve clarity and reduce ambiguity in team communications?"<br>- "How might we enable seamless information flow that prevents misunderstandings and optimizes effort?" |
| Remote team members feel disconnected from the rest of the team. | - "How might we foster a stronger sense of connection and belonging among remote team members?"<br>- "How might we enhance virtual interactions to build a cohesive and engaged remote team?" |

#### Lateral Thinking

**Core principle:** Generate new ideas by looking at problems from unexpected angles, deliberately breaking conventional thought patterns.

**Cognitive mode alignment:** Generative (Random Entry, Movement), Provocative (PO, Challenge)

##### Principles of Question Design

Lateral Thinking, developed by Edward de Bono, is about generating new ideas and solving problems by looking at them from unexpected angles, often by deliberately breaking conventional thought patterns. Its techniques are designed to disrupt linear thinking and encourage novel connections. Key question techniques include Random Entry, which introduces a random word, object, or image to stimulate new associations and perspectives related to the problem. Provocation (PO) involves making a deliberately absurd or impossible statement to challenge assumptions and generate new ideas that might not otherwise emerge, often starting with "PO" (Provocative Operation). Movement uses a provocation as a stepping stone to generate new ideas, rather than evaluating its practicality directly, focusing on moving forward from the unusual. Challenge involves questioning existing assumptions, rules, or ways of doing things, even those that seem obvious or fundamental, to open up new possibilities [4].

##### Examples of Questions for a Generic Brainstorming Topic (e.g., "Improving Team Collaboration")

The following table presents examples of Lateral Thinking questions for improving team collaboration:

| Lateral Thinking Technique | Example Questions |
| :--- | :--- |
| **Random Entry** (using the word "cloud") | - "How does a 'cloud' relate to our team collaboration? What ideas does that spark?"<br>- "If our team collaboration were a 'cloud,' what kind of cloud would it be, and what does that tell us about how to improve it?"<br>- "What elements of 'cloud' technology or nature could inspire new ways of collaborating?" |
| **Provocation (PO)** | - "PO: All team meetings must be conducted in silence." (What ideas does this absurd rule provoke about communication or non-verbal cues?)<br>- "PO: Every team member must disagree with the last idea proposed." (What does this provoke about critical thinking or alternative perspectives?) |
| **Movement** (from the "silent meeting" provocation) | - "If meetings were silent, how would we ensure understanding? This makes me think about visual communication tools or pre-reading materials. How can we integrate more of these?"<br>- "What if we used a silent period at the start of every meeting for individual reflection before discussion?" |
| **Challenge** | - "Why do we always start our projects with a detailed plan? What if we started with a minimal viable product instead?"<br>- "Why do we assume that team collaboration must happen synchronously? What if asynchronous collaboration was our primary mode?"<br>- "What if we challenged the notion that all decisions require full team consensus?" |

#### Six Thinking Hats

**Core principle:** Parallel thinking allowing exploration from different perspectives without conflict; each hat represents a different mode of thinking.

**Cognitive mode alignment:** White (Analytical), Red (Human-Centered/Emotional), Black (Analytical/Risk-focused), Yellow (Generative/Optimistic), Green (Generative/Creative), Blue (Analytical/Meta)

##### Principles of Question Design

Edward de Bono's Six Thinking Hats is a powerful tool for parallel thinking, allowing individuals and teams to explore a topic from different perspectives without conflict. Each hat represents a different mode of thinking, and by consciously switching hats, a comprehensive view can be achieved. When applied to question generation, each hat guides the type of questions asked:

*   **White Hat (Facts & Information):** Focuses on objective facts, figures, and information. Questions seek data, evidence, and verifiable truths.
*   **Red Hat (Feelings & Intuition):** Focuses on emotions, feelings, and intuition without justification. Questions explore gut reactions, likes, dislikes, and emotional responses.
*   **Black Hat (Caution & Critical Judgment):** Focuses on risks, problems, weaknesses, and potential negative outcomes. Questions identify flaws, dangers, and reasons why something might not work.
*   **Yellow Hat (Optimism & Benefits):** Focuses on positive aspects, benefits, value, and opportunities. Questions explore advantages, feasibility, and potential gains.
*   **Green Hat (Creativity & New Ideas):** Focuses on new ideas, alternatives, possibilities, and creative solutions. Questions encourage divergent thinking, brainstorming, and innovation.
*   **Blue Hat (Process Control & Organization):** Focuses on managing the thinking process itself. Questions guide the discussion, set agendas, and summarize conclusions [5].

##### How Red Hat Questions Differ from Green Hat Questions

**Red Hat questions** are subjective and emotional, exploring feelings, instincts, and gut reactions without requiring logic or justification. They are about expressing emotions related to the topic. In contrast, **Green Hat questions** are creative and generative, aiming to produce new ideas, alternatives, and possibilities. They are about exploring new directions and innovative solutions.

##### Examples of Questions for a Generic Brainstorming Topic (e.g., "Improving Team Collaboration")

The following table provides examples of questions for each of the Six Thinking Hats, applied to the topic of improving team collaboration:

| Thinking Hat | Focus | Example Questions |
| :--- | :--- | :--- |
| **White Hat** | Facts & Information | - "What are the current statistics on our team's project completion rates and deadlines?"<br>- "What communication tools are currently in use by the team, and how frequently are they utilized?"<br>- "What are the documented procedures for conflict resolution within the team?" |
| **Red Hat** | Feelings & Intuition | - "How do you *feel* about the current level of team collaboration?"<br>- "What is your gut reaction to the idea of implementing a new collaboration platform?"<br>- "What concerns or excitements do you *sense* regarding potential changes to our team dynamics?" |
| **Black Hat** | Caution & Critical Judgment | - "What are the potential risks or downsides of changing our current collaboration methods?"<br>- "Where could a new collaboration initiative fail, and what obstacles might we encounter?"<br>- "What are the weaknesses in our current team structure that could hinder improved collaboration?" |
| **Yellow Hat** | Optimism & Benefits | - "What are the potential benefits of fostering stronger team collaboration?"<br>- "How could improved collaboration positively impact our project outcomes and team morale?"<br>- "What opportunities might arise if our team collaboration reached an exemplary level?" |
| **Green Hat** | Creativity & New Ideas | - "What entirely new ways could we imagine for our team to collaborate that we haven't considered before?"<br>- "If there were no constraints, how would we reinvent team collaboration from scratch?"<br>- "What alternative approaches to team communication could spark breakthrough ideas?" |
| **Blue Hat** | Process Control & Organization | - "What is our objective for this brainstorming session on team collaboration?"<br>- "How should we structure our discussion to ensure we cover all perspectives on collaboration?"<br>- "What conclusions have we reached so far regarding improving team collaboration?" |

### 1.3 Cognitive Diversity and Question Quality

#### 1.3.0 Summary

**Research consensus:** Teams with diverse cognitive styles among question-generators produce better ideation outcomes than homogeneous groups.

**Key findings:**

1. **Question diversity drives solution diversity:** Diverse questions, particularly those that challenge assumptions and explore problems from multiple angles, stimulate a broader range of cognitive processes, leading to a wider array of potential solutions.

2. **Analytical-only framing constrains creative output:** When innovators are compelled to operate within a purely adaptive/analytical framework, their perceived creativity decreases. Over-reliance on analytical framing limits the breadth and novelty of generated ideas.

3. **Critical cognitive modes for question-generation teams:**
   - **Adaptive Thinking:** Structured, incremental approaches; refining existing solutions; ensuring questions are well-defined and practical
   - **Innovative Thinking:** Broad, unstructured approaches; challenging assumptions; generating novel solutions; producing boundary-pushing questions
   - **Recall/Knowledge-based:** Questions assessing explicit factual information and basic concepts
   - **Inferential:** Questions requiring connecting disparate information, drawing logical conclusions
   - **Synthetical/Main Idea:** Questions demanding understanding of overarching themes and integration of multiple concepts

4. **Frameworks for cognitive diversity in questions:**
   - **Bloom's Taxonomy:** Six levels of cognitive complexity (Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation)
   - **ReQUESTA Framework:** Specialized agents for three cognitive modes (Text-based/Recall, Inferential, Main Idea/Synthesis)
   - **Kirton's Adaption-Innovation Theory:** Categorizes individuals as Adaptors (prefer structured approaches) or Innovators (prefer unstructured, novel approaches)

**Critical implication:** A question-generation team composed exclusively of analytical/adaptive thinkers will systematically underrepresent innovative, creative, and imaginative question modes, constraining downstream ideation.

#### 1.3.1. Introduction

This research investigates the critical role of **cognitive diversity** in the process of **question generation** and its subsequent influence on **ideation outcomes**, particularly within professional brainstorming and facilitated dialogue contexts. The study aims to explore whether diverse cognitive styles among question-generators lead to superior ideation, the relationship between question diversity and solution diversity, instances where analytical-only question framing has constrained creative output, and the cognitive modes essential for effective question-generation teams. This inquiry is particularly relevant for evaluating brainstorming systems that utilize isolated subagents, where the quality and diversity of generated questions are paramount for fostering innovative solutions.

#### 1.3.2. Defining Cognitive Diversity

**Cognitive diversity** encompasses the varied ways individuals perceive, process, and apply information, influencing their problem-solving approaches, decision-making patterns, and overall thinking styles [1] [2]. It extends beyond demographic differences to include variations in expertise, experiences, information interpretation, preferences, and fundamental ways of thinking within a team [2]. These diverse cognitive styles can arise from a multitude of sources, such as educational background, professional history, personality types, personal values, life experiences, and even demographic characteristics, which can often serve as a proxy for cognitive differences [2].

Key cognitive styles relevant to ideation and question generation include, but are not limited to, distinctions between optimistic versus pessimistic outlooks, risk-averse versus risk-tolerant dispositions, preferences for quantitative versus qualitative information, extroverted versus introverted communication styles, detail-oriented versus high-level strategic thinking, contrarian versus consensus-oriented approaches, and long-term versus short-term perspectives [2]. Recognizing and leveraging these varied cognitive dimensions is fundamental to understanding their impact on the quality and breadth of generated questions and subsequent ideation.

#### 1.3.3. Impact of Cognitive Diversity on Question Generation and Ideation

Cognitive diversity profoundly influences the effectiveness of question generation and the richness of ideation outcomes through several interconnected channels, including the generation and sharing of ideas, coordination among team members, and team affinity [2].

##### 1.3.3.1. Does diverse cognitive styles among question-generators produce better ideation outcomes than homogeneous ones?

Research consistently suggests that teams exhibiting diverse cognitive styles among question-generators tend to produce **better ideation outcomes** compared to homogeneous groups [1] [2]. This superiority stems from the broader spectrum of knowledge, viewpoints, and problem-solving approaches that diverse teams bring to the table. For instance, a cognitively diverse team is better equipped to identify and overcome inherent biases, such as confirmation or familiarity bias, as the presence of varied perspectives can lead to the cancellation of individual biases, resulting in more objective and comprehensive idea generation [2]. The "Wisdom of Crowds" phenomenon exemplifies this, where the collective intelligence of a diverse group often surpasses that of individual experts, leading to highly accurate outcomes even when individual contributions may be flawed [2].

Furthermore, diverse cognitive styles enhance problem-solving by allowing for a more thorough and well-rounded exploration of challenges. Analytical thinkers might meticulously examine data and patterns, while creative thinkers could propose novel solutions previously unconsidered. The synergy between these different thinking styles enables the generation of ideas that are both innovative and practical [1]. In the context of a brainstorming system with isolated subagents, the inherent diversity of these subagents, each with its unique processing and generation capabilities, is expected to yield a wider array of questions and, consequently, more varied and creative ideation outcomes, even in the absence of real-time group discussion [2].

##### 1.3.3.2. What is the relationship between question diversity and solution diversity?

A direct and significant relationship exists between **question diversity** and **solution diversity**. Diverse questions, particularly those that challenge assumptions and explore problems from multiple angles, are instrumental in stimulating a broader range of cognitive processes, which in turn leads to a wider array of potential solutions [3]. The way a problem or question is framed can either expand or constrain the creative output, depending on the cognitive style of the individual or subagent addressing it [3].

For example, studies on design ideation indicate that innovative cognitive styles and problem framings correlate with higher perceptions of idea diversity and creativity. Conversely, adaptive (more analytical) framings can lead to lower perceptions of creativity and increased difficulty in generating ideas [3]. This implies that a rich and varied set of questions, encompassing different cognitive demands, will naturally encourage a broader spectrum of responses and solutions. If a brainstorming system can generate questions that span various cognitive levels—from factual recall to complex inference and synthesis—it will inherently promote a more diverse and innovative set of solutions from the participating subagents [4].

##### 1.3.3.3. Are there documented cases where analytical-only question framing constrained creative output?

While explicit, documented cases of "analytical-only question framing" directly constraining creative output are often implicitly rather than explicitly stated in literature, the underlying principles are well-supported [3]. The research on problem framing and cognitive styles provides strong evidence that an over-reliance on adaptive or analytical framing can indeed limit creative output, especially for individuals with innovative cognitive styles [3]. When innovators are compelled to operate within a purely adaptive framework, their perceived creativity tends to decrease, suggesting that rigid, analytical-only approaches to question framing can restrict the breadth and novelty of generated ideas [3].

For instance, in the context of automated question generation, large language models (LLMs) often excel at producing lower-level recall or analytical questions without specific control mechanisms. However, their performance degrades significantly when tasked with generating questions that require higher-order cognitive skills like inference or synthesis [4]. This demonstrates that if question generation is confined to an analytical-only approach, it will inherently limit the cognitive demands placed on respondents, thereby curtailing the potential for creative and inferential ideation. The intentional design of frameworks like ReQUESTA to generate questions across various cognitive levels is a direct response to this recognized limitation of undifferentiated, often analytical, question generation [4].

##### 1.3.3.4. What cognitive modes are most important to represent in a question-generation team?

To foster comprehensive and innovative ideation, a question-generation team, whether human or agent-based, should represent a diverse array of cognitive modes. Based on the research, the most important cognitive modes to represent include:

*   **Adaptive Thinking:** Characterized by a preference for structured, incremental approaches to problem-solving and a focus on refining existing solutions. This mode is crucial for ensuring questions are well-defined, practical, and grounded in existing knowledge [3].
*   **Innovative Thinking:** Defined by a preference for broad, unstructured approaches, challenging assumptions, and generating novel solutions. This mode is essential for producing questions that push boundaries, encourage divergent thinking, and open new avenues for exploration [3].
*   **Recall/Knowledge-based:** The ability to generate questions that assess explicit factual information and basic concepts. These questions form the foundation for understanding and are critical for ensuring a shared baseline of knowledge [4].
*   **Inferential:** The capacity to formulate questions that require connecting disparate pieces of information, drawing logical conclusions, and understanding implied meanings. These questions move beyond surface-level comprehension and encourage deeper analytical thought [4].
*   **Synthetical/Main Idea:** The skill to create questions that demand an understanding of overarching themes, central arguments, or the integration of multiple concepts into a coherent whole. These questions are vital for fostering higher-order thinking and holistic understanding [4].

By integrating these cognitive modes, a question-generation team can produce a balanced set of inquiries that not only explore the known but also venture into the unknown, leading to richer and more diverse ideation outcomes. The interplay between these styles, and the ability to frame problems (or questions) in ways that resonate with different cognitive preferences, is key to maximizing both idea diversity and creativity [3].

#### 1.3.4. Key Frameworks and Taxonomies

Several frameworks and taxonomies are instrumental in understanding and guiding cognitively diverse question generation.

##### 1.3.4.1. General Cognitive Diversity Frameworks

While not exclusively for question generation, general cognitive diversity frameworks provide the foundational understanding of different thinking styles. Kirton's Adaption-Innovation (A-I) theory, for example, categorizes individuals as either Adaptors or Innovators, highlighting their preferred approaches to change and problem-solving [3]. Other frameworks may categorize individuals based on their preferences for quantitative vs. qualitative information, risk tolerance, or communication styles [2]. These frameworks help in identifying and understanding the inherent cognitive differences within a team.

##### 1.3.4.2. Question Generation Frameworks (e.g., Bloom's Taxonomy, ReQUESTA)

###### Bloom's Taxonomy

**Bloom's Taxonomy** is a widely recognized hierarchical framework that classifies learning objectives into six levels of cognitive complexity, providing a robust tool for crafting questions that target specific intellectual skills [5]. By aligning question generation with these levels, educators and facilitators can ensure a cognitively diverse set of inquiries that stimulate various thought processes. The levels are:

1.  **Knowledge (Remembering):** Focuses on recalling facts and basic concepts. 
    *   *Example Question:* "What are the primary components of cognitive diversity?" [5]
2.  **Comprehension (Understanding):** Involves interpreting and explaining ideas or concepts. 
    *   *Example Question:* "Explain how cognitive diversity differs from demographic diversity." [5]
3.  **Application (Applying):** Requires using knowledge in new situations. 
    *   *Example Question:* "How can a team apply principles of cognitive diversity to improve a brainstorming session?" [5]
4.  **Analysis (Analyzing):** Involves breaking down information into parts and identifying relationships. 
    *   *Example Question:* "Analyze the potential causes for mixed empirical evidence regarding cognitive diversity's impact." [5]
5.  **Synthesis (Creating):** Focuses on combining elements to form a new whole or proposing alternative solutions. 
    *   *Example Question:* "Propose a novel approach to question generation that integrates both adaptive and innovative cognitive styles." [5]
6.  **Evaluation (Evaluating):** Involves making judgments based on criteria and standards. 
    *   *Example Question:* "Evaluate the effectiveness of psychological safety in fostering diverse question generation." [5]

By systematically generating questions across these levels, a team can ensure that ideation is not confined to superficial recall but extends to deeper understanding, critical analysis, creative problem-solving, and informed judgment.

###### ReQUESTA Framework

The **ReQUESTA (Reading Question-generation Using Educational Smart Text Agents)** framework is a hybrid multi-agent system specifically designed for generating cognitively diverse multiple-choice questions [4]. It explicitly targets cognitive diversity in generated artifacts by reflecting differences in the reasoning processes required for a successful response. ReQUESTA employs specialized agents to generate questions across three key cognitive modes:

1.  **Text-based (Recall):** Questions that target explicit factual information. 
    *   *Example Question:* "According to the text, what is the primary function of the Preprocessor agent in ReQUESTA?" [4]
2.  **Inferential:** Questions that require integrating information beyond surface-level recall to draw conclusions. 
    *   *Example Question:* "Based on the ReQUESTA framework, what can be inferred about the challenges of single-pass LLM question generation?" [4]
3.  **Main Idea (Synthesis):** Questions that assess understanding of overarching themes or central arguments. 
    *   *Example Question:* "What is the central argument presented by the ReQUESTA framework regarding controlled question generation?" [4]

ReQUESTA's modular and agentic design demonstrates a practical approach to achieving cognitive diversity in question generation, particularly relevant for automated systems. It highlights the importance of explicitly designing for different cognitive demands to ensure a comprehensive and varied output [4].

#### 1.3.5. Conclusion

Cognitive diversity among question-generators is a critical determinant of the quality and breadth of downstream ideation. Diverse cognitive styles foster richer idea generation, mitigate biases, and enhance problem-solving capabilities. A direct correlation exists between the diversity of questions asked and the diversity of solutions generated, with frameworks like Bloom's Taxonomy and ReQUESTA providing structured approaches to ensure cognitive breadth in question formulation. Cases where analytical-only question framing constrained creative output underscore the necessity of incorporating varied cognitive modes, including both adaptive and innovative thinking, as well as questions targeting recall, inference, and synthesis. For brainstorming systems, particularly those utilizing isolated subagents, intentionally cultivating cognitive diversity in question generation is paramount for unlocking truly innovative and comprehensive ideation outcomes.

### 1.4 Question Quality Dimensions

#### Summary

A comprehensive rubric for evaluating brainstorming questions:

| Dimension | Criteria for Good Question | Poor Example | Good Example |
|:----------|:---------------------------|:-------------|:-------------|
| **Open-endedness** | Encourages wide range of responses; avoids yes/no or single-answer prompts | "Should we add a new feature?" | "How might we enhance user engagement with our product?" |
| **Constraint (Balanced)** | Broad enough for diverse ideas, yet focused enough to be relevant; avoids too broad or too narrow/solution-prescriptive | "How might we redesign everything?" (too broad) / "How might we implement a chatbot?" (too narrow) | "How might we improve customer satisfaction with our support channels?" |
| **Specificity (Impact on Diversity)** | Specific enough to address clear problem, but not so specific that it limits response diversity | "How might we tell users which form to complete?" | "How might we make users feel confident they are filing their taxes correctly?" |
| **Assumptions (Freedom from)** | Challenges existing assumptions; does not pre-suppose particular solution or problem framing | "How can we optimize our existing database queries?" | "What alternative approaches could we take to manage our data efficiently?" |
| **Invitation (Analysis vs. Imagination)** | Primarily invites imaginative thinking and exploration of hypotheticals in early ideation; balances with analytical questions for refinement | "What are the current market trends?" (purely analytical) | "What if our product could anticipate user needs before they arise?" |
| **Positive Framing** | Phrased positively, focusing on desired outcomes rather than negative aspects or problems to eliminate | "How might we reduce customer complaints?" | "How might we increase customer delight?" |

**Key insight:** Questions that invite **imagination** (divergent, data-poor, hypothetical scenarios) are fundamentally different from questions that invite **analysis** (defined scope, breaking down problems, logical reasoning based on existing data). Effective brainstorming requires both, but imaginative questions are critical for generating novel ideas, while analytical questions are better suited for refinement and evaluation.

#### Introduction

This research investigates the critical dimensions that define a "good" brainstorming question within professional brainstorming and facilitated dialogue contexts. The objective is to understand how question design influences the effectiveness of ideation, the diversity of responses, and the creativity of outcomes. This document synthesizes findings from established frameworks, academic research, and practitioner guidance to develop a rubric for evaluating brainstorming questions.

#### Key Question Quality Dimensions

##### 1. Open-ended vs. Closed Question Effectiveness in Ideation

**Open-ended questions** are crucial for fostering divergent thinking and generating a wide array of ideas in brainstorming and ideation contexts. The "How Might We" (HMW) framework, for instance, advocates for questions that are broad enough to encourage numerous creative ideas without suggesting a specific solution [1]. This approach inherently promotes open-endedness, allowing participants to explore diverse solution pathways rather than being constrained to a narrow set of predefined answers.

Conversely, **closed questions**, which typically elicit a 'yes' or 'no' or a very specific piece of information, tend to limit the scope of responses and can stifle creativity. While useful for gathering specific data, they are generally less effective in the initial ideation phases of brainstorming where the goal is to maximize idea generation and exploration.

##### 2. The Role of Constraint in Question Design

Question design requires a delicate balance in terms of **constraint**. Questions that are **too broad** can lead to unfocused ideation, where responses lack direction and relevance to the core problem. For example, a question like "How might we redesign the submission-drafting process?" was deemed too broad as it loses sight of the specific problem of checking for mistakes [1]. Such overly broad questions can dilute efforts and make it difficult to converge on actionable insights.

Conversely, questions that are **too narrow** or embed solutions can be overly limiting, restricting the pool of possibilities and hindering creative problem-solving. The Nielsen Norman Group emphasizes avoiding HMW questions that suggest a solution, as this restricts the types of ideas generated [1]. For instance, asking "How might we tell users which form to complete to file their taxes?" is too narrow as it pre-supposes a communication-based solution, overlooking other potential approaches like automatic filing or simplified forms [1].

The optimal approach involves crafting questions that are **broad enough to encourage many creative ideas, yet specific enough to remain focused on the desired outcome or root problem** [1]. This balanced constraint guides ideation without imposing undue limitations, fostering both creativity and relevance.

##### 3. How Question Specificity Affects Response Diversity

**Question specificity** plays a direct role in influencing the **diversity of responses** during ideation. Highly specific questions, especially those that embed solutions or are too narrow, tend to restrict the range of possible answers, leading to less diverse outcomes [1]. For instance, a question focused on a particular technical solution will likely yield ideas only within that technical domain, neglecting broader, potentially more innovative solutions.

Conversely, **divergent questions**, characterized by their “data-poor” nature, are designed to maximize response diversity [3]. By presenting scenarios with insufficient information for a single correct answer, these questions compel individuals to make assumptions and explore different paths, naturally leading to a wider range of ideas [3].

##### 4. Questions with Embedded Assumptions and Creative Responses

Questions with **embedded assumptions** can significantly impact the creativity and diversity of responses. When a question contains implicit assumptions, it can lead to **fixation**, where participants become stuck on a particular line of thought and fail to explore alternative possibilities [2]. For example, a question like "How can we improve the efficiency of our current software by optimizing its database queries?" assumes that the current software and its database are the only viable solution space, potentially overlooking a complete re-architecture or a different technological approach [2].

To foster creativity, questions should be framed to **challenge existing assumptions** and encourage divergent thinking. The "How Might We" framework directly addresses this by warning against solution-suggesting questions, which are a form of embedded assumption [1]. By avoiding such assumptions, questions can open up a wider range of potential solutions and lead to more innovative outcomes.

##### 5. Questions that Invite Analysis vs. Questions that Invite Imagination

The distinction between questions that invite **analysis** versus those that invite **imagination** is crucial in brainstorming and ideation. **Analytical questions** typically have a more defined scope and aim to break down a problem into its constituent parts, often leading to a single or limited set of correct answers based on existing data or logical reasoning. These questions are valuable for problem clarification and evaluation but can be less effective for generating novel ideas.

In contrast, **imaginative questions**, such as the divergent questions described by Byrdseed, are designed to spark creativity and exploration of the unknown [3]. These questions often present hypothetical scenarios or "data-poor" situations where participants must fill in the gaps and make creative leaps. For example, asking "What if Spain had not been defeated when the Armada was destroyed in 1588, and Spain went on to conquer England. What would the world be like today?" invites a far more imaginative and exploratory response than a purely analytical question about the historical event [3].

For effective brainstorming, a combination of both types of questions may be necessary. Imaginative questions can be used to generate a wide range of initial ideas, while analytical questions can then be used to refine, evaluate, and select the most promising concepts.

#### Rubric for Evaluating Brainstorming Questions

Based on the identified quality dimensions, the following rubric can be used to evaluate brainstorming questions, particularly in contexts where isolated subagents are generating responses. This rubric aims to guide the design of questions that foster creativity, diversity, and relevance.

| Dimension | Criteria for a Good Question | Poor Question Example | Good Question Example |
|---|---|---|---|
| **Open-endedness** | Encourages a wide range of responses; avoids yes/no or single-answer prompts. | "Should we add a new feature?" | "How might we enhance user engagement with our product?" |
| **Constraint (Balanced)** | Broad enough for diverse ideas, yet focused enough to be relevant to the problem; avoids being too broad or too narrow/solution-prescriptive. | "How might we redesign everything?" (Too broad) <br> "How might we implement a chatbot for customer support?" (Too narrow/solution-prescriptive) | "How might we improve customer satisfaction with our support channels?" [1] |
| **Specificity (Impact on Diversity)** | Specific enough to address a clear problem or insight, but not so specific that it limits response diversity; avoids embedding solutions. | "How might we tell users which form to complete?" [1] | "How might we make users feel confident they are filing their taxes correctly?" [1] |
| **Assumptions (Freedom from)** | Challenges existing assumptions; does not pre-suppose a particular solution or problem framing. | "How can we optimize our existing database queries?" [2] | "What alternative approaches could we take to manage our data efficiently?" [2] |
| **Invitation (Analysis vs. Imagination)** | Primarily invites imaginative thinking and exploration of hypotheticals, especially in early ideation phases; balances with analytical questions for refinement. | "What are the current market trends?" (Purely analytical) | "What if our product could anticipate user needs before they arise?" [3] |
| **Positive Framing** | Phrased positively, focusing on desired outcomes rather than negative aspects or problems to be eliminated. | "How might we reduce customer complaints?" [1] | "How might we increase customer delight?" [1] |

#### Conclusion

This research has explored the multifaceted dimensions of question quality in brainstorming and facilitated dialogue, highlighting their critical role in shaping ideation outcomes. The synthesis of findings from the "How Might We" framework, a critical review of Osborn's brainstorming assumptions, and the concept of divergent questions reveals that effective question design is a deliberate process that balances open-endedness with appropriate constraint, avoids embedded assumptions, and strategically invites either analytical or imaginative thinking based on the ideation phase.

The developed rubric provides a structured approach for evaluating brainstorming questions, ensuring they are crafted to maximize creativity, diversity, and relevance, especially in environments with isolated subagents. By adhering to these quality dimensions, facilitators and system designers can significantly enhance the effectiveness of brainstorming sessions, leading to more innovative and actionable solutions.

### 1.5 Bias in Question Generation

#### Summary

**Anchoring effects:** Early questions act as powerful anchors, constraining the range and direction of ideas generated. The first questions asked set a narrow frame of reference, making it difficult for participants to think outside that frame.

**Example:** If a brainstorming session on improving a product begins with "How can we reduce manufacturing costs?", subsequent ideas will likely focus on cost-saving measures, while potentially more innovative ideas about new features or markets are neglected.

**Framing bias:**

- **Analytical framing suppresses creative responses:** Questions like "What are the logistical challenges of launching a new product in Europe?" encourage detailed analytical breakdown but may not inspire creative solutions.
- **Creative framing suppresses analytical rigor:** Questions like "If our product were a superhero, what would its superpower be?" can spark imaginative ideas but may not lead to practical, actionable solutions.

**Question-response loop:** The type of questions asked predetermines the type of answers generated, creating a self-reinforcing cycle.

- **"Why" questions** → Explanations, root causes, justifications, blame-oriented discussion
- **"How" questions** → Practical steps, methodologies, actionable solutions
- **"What if" questions** → Speculative, creative thinking, breaking free from constraints

**Critical implication for isolated subagents:** In systems where subagents operate with isolated context windows, the initial prompt (question) given to each subagent heavily influences the output it generates. If all question-generating personas use analytical framing, the question-response loop will systematically bias all downstream brainstorming toward analytical territory.

**Evidence-based countermeasures:**

1. **Use broad opening questions** that do not presuppose a particular direction
2. **Present multiple, simultaneous questions** to diffuse anchoring effect
3. **Re-frame problems from different perspectives** (customer, competitor, unrelated industry)
4. **Alternate between question types** to encourage both analytical and creative thinking
5. **Question brainstorming** (brainstorm questions before ideas)
6. **Anonymous idea submission and silent voting** to counteract conformity bias

#### Introduction

This document explores the systematic biases that can be introduced into brainstorming and facilitated dialogue through the framing of questions. Specifically, it examines anchoring effects, framing bias, and the question-response loop. It also provides evidence-based countermeasures from facilitation practice to mitigate these biases. The insights are particularly relevant for understanding and improving brainstorming systems, including those that use AI-powered subagents.

#### 1. Anchoring Effect in Question Generation

The anchoring effect is a cognitive bias where individuals rely too heavily on an initial piece of information (the "anchor") when making subsequent judgments. In the context of brainstorming, the first questions asked can act as powerful anchors, constraining the range and direction of the ideas generated.

##### 1.1. How Anchoring Constrains Thinking

Early questions can set a narrow frame of reference, making it difficult for participants to think outside of that frame. For example, if a brainstorming session on improving a product begins with a question about "reducing manufacturing costs," subsequent ideas are likely to focus on cost-saving measures, while potentially more innovative ideas about new features or markets are neglected. This is because the initial question anchors the discussion to a specific aspect of the problem, even if that aspect is not the most critical one to address.

Research by Tversky and Kahneman demonstrated that even arbitrary numerical anchors can significantly influence estimates. In one study, participants were asked to estimate the percentage of African countries in the United Nations after a roulette wheel was spun. The number on the wheel, although random, acted as an anchor, influencing their estimates. [1]

##### 1.2. Counteracting Anchoring Effects

Facilitators can employ several techniques to counteract the anchoring effect:

*   **Use Broad Opening Questions:** Start with broad, open-ended questions that do not presuppose a particular direction. For example, instead of asking "How can we make our app more user-friendly?", a facilitator could ask "What are all the ways we can improve the user experience of our app?"
*   **Multiple, Simultaneous Questions:** Presenting multiple questions at once can diffuse the anchoring effect of any single question. This encourages participants to think about the problem from different perspectives.
*   **Silent Brainstorming:** Allowing individuals to generate ideas silently before sharing them with the group can reduce the influence of the first ideas spoken, which can act as anchors for the rest of the group.

#### 2. Framing Bias in Question Generation

Framing bias refers to the way in which the presentation of information, including the wording of a question, can influence how people respond. The framing of a question can be either positive or negative, or it can be framed to encourage either analytical or creative thinking.

##### 2.1. Analytical vs. Creative Framing

*   **Analytical Framing:** Questions framed analytically tend to suppress creative responses. For example, a question like "What are the logistical challenges of launching a new product in Europe?" encourages a detailed, analytical breakdown of the problem, but it may not inspire creative solutions.
*   **Creative Framing:** Conversely, questions framed creatively can suppress analytical rigor. A question like "If our product were a superhero, what would its superpower be?" can spark imaginative ideas, but it may not lead to practical, actionable solutions.

##### 2.2. The Question-Response Loop: Predetermining Answers

The **question-response loop** highlights how the initial type of questions asked can significantly predetermine the nature of the answers generated, thereby shaping the entire brainstorming or dialogue outcome. This creates a self-reinforcing cycle where the framing of questions limits the scope of possible responses.

**Examples:**

*   **"Why" Questions:** A series of "Why did this fail?" questions will primarily elicit explanations, root causes, and justifications, focusing on past problems and potentially leading to a blame-oriented discussion. This framing might suppress forward-looking or innovative solutions.
*   **"How" Questions:** Conversely, questions like "How can we improve this process?" will generate practical steps, methodologies, and actionable solutions, focusing on implementation. While useful, an exclusive focus on "how" might overlook the fundamental "why" or broader strategic considerations.
*   **"What if" Questions:** These questions encourage speculative and creative thinking, such as "What if we had unlimited resources?" or "What if our primary competitor ceased to exist?" They are designed to break free from current constraints and generate novel ideas, but without subsequent analytical framing, these ideas might lack practical grounding.

This loop is particularly critical in systems where subagents operate with isolated context windows, as the initial prompt (question) given to each subagent will heavily influence the output it generates, potentially leading to a narrow range of responses if not carefully managed.

##### 2.3. Counteracting Framing Bias

*   **Re-framing:** Facilitators can guide the group to re-frame the problem from different perspectives. This can involve asking the group to consider the problem from the perspective of a customer, a competitor, or even a completely unrelated industry.
*   **Alternating Question Types:** A skilled facilitator will alternate between different types of questions to encourage both analytical and creative thinking. For example, a session might start with a creative framing to generate a wide range of ideas, and then move to a more analytical framing to evaluate and refine those ideas.

#### 3. Facilitation Techniques for Counteracting Question Bias

In addition to the specific techniques mentioned above, there are several general facilitation practices that can help to counteract bias in question generation:

*   **Question Brainstorming:** Instead of brainstorming ideas, brainstorm questions. This technique, also known as "question-storming," can help to uncover a wider range of issues and perspectives before diving into solutions.
*   **Devil's Advocate:** Assigning someone to play the role of a devil's advocate can help to challenge assumptions and uncover hidden biases in the way the problem is being framed.
*   **Anonymous Idea Submission and Silent Voting:** Anonymous brainstorming, coupled with silent voting, can effectively counteract biases stemming from individuals' motivations to conform [2]. In traditional brainstorming, the first ideas presented or those from more influential individuals can anchor the discussion and lead to conformity. By allowing participants to submit ideas anonymously, the focus shifts from the source of the idea to the idea itself, fostering a more equitable evaluation process. Silent voting further reinforces this by enabling independent assessment of ideas, free from social pressures.

#### Conclusion

Bias in question generation is a significant challenge in brainstorming and facilitated dialogue. Anchoring effects and framing bias can constrain thinking and predetermine the outcome of a session. However, by being aware of these biases and employing a range of facilitation techniques, it is possible to mitigate their effects and foster a more open, creative, and productive brainstorming environment. These principles are equally applicable to human-led facilitation and the design of AI-powered brainstorming systems.

---

## 2. Persona Coverage Mapping

### 2.1 Mapping Table

This table maps all 22 Idea Symphony personas against the question taxonomy and framework findings, assessing their natural question type, framework alignment, dimension emphasis, and question-generation fitness.

| Persona | Primary Mode | Natural Question Type | Framework Alignment | Dimension Emphasis | Question-Generation Fitness |
|:--------|:-------------|:---------------------|:-------------------|:-------------------|:---------------------------|
| **The Accountant** | Specialized Lens | Evaluative, Convergent | White Hat (facts/data), CPS (fact-finding) | Specificity, Analysis-invitation | **Medium** — Better at answering financial questions than generating exploratory ones; questions tend toward "What does this cost?" rather than "What possibilities exist?" |
| **The Analyst** | Analytical | Convergent, Evaluative, Socratic (all categories except viewpoints) | White Hat, Black Hat, CPS (problem-finding), Socratic Method | Constraint (narrow), Analysis-invitation, Assumptions (probing) | **Low** — Natural at answering analytical questions, but generates questions that are too convergent and evaluative for divergent ideation; retired from Phase 3 for lack of distinctiveness |
| **The Analogist** | Connective | Divergent, Socratic (questioning viewpoints) | Green Hat (creative), Lateral Thinking (random entry), CPS (ideate) | Open-endedness, Imagination-invitation, Freedom from assumptions | **High** — Naturally generates cross-domain questions like "How does X work in Y field?" and "What if we imported Z approach?"; excellent at opening new question territory |
| **The Audience Advocate** | Human-Centered | Generative, Socratic (clarification, questioning viewpoints) | Red Hat (feelings), Design Thinking (HMW), CPS (fact-finding) | Open-endedness, Positive framing, Specificity (user-focused) | **High** — Naturally generates empathic, user-focused questions like "What challenges do users face?" and "How might we make users feel...?"; strong HMW alignment |
| **The Connector** | Connective | Divergent, Socratic (questioning viewpoints) | Green Hat (creative), Lateral Thinking (random entry), CPS (ideate) | Open-endedness, Imagination-invitation | **High** — Generates structural parallel questions like "What patterns from X apply to Y?" and "How are these seemingly different things related?"; opens connective question territory |
| **The Constraint Flipper** | Generative | Divergent, Generative | Green Hat (creative), Lateral Thinking (movement, challenge), CPS (ideate) | Freedom from assumptions, Imagination-invitation, Positive framing | **High** — Naturally generates reframing questions like "What if this limitation were an advantage?" and "How could we turn this constraint into an opportunity?"; excellent at challenging embedded assumptions |
| **The Devil's Advocate** | Analytical | Convergent, Evaluative, Socratic (probing assumptions, implications) | Black Hat (caution/risks), CPS (develop/evaluate) | Analysis-invitation, Assumptions (probing), Constraint (narrow) | **Medium** — Generates important risk-probing questions, but these are evaluative/convergent rather than generative; better suited to evaluating existing ideas than opening new territory |
| **The Empath** | Human-Centered | Generative, Divergent | Red Hat (feelings/intuition), Appreciative Inquiry (all phases), Design Thinking (HMW) | Open-endedness, Positive framing, Imagination-invitation | **Very High** — Naturally generates emotionally resonant questions like "How would this make people feel?" and "What would bring joy/meaning/connection?"; only persona naturally aligned with Appreciative Inquiry's affirmative framing |
| **The First Principles Thinker** | Analytical | Convergent, Socratic (probing assumptions, reasons/evidence) | White Hat (facts), Socratic Method, CPS (formulate challenges) | Assumptions (probing), Analysis-invitation, Constraint (narrow) | **Medium** — Generates foundational questions like "What are we assuming?" and "What is fundamentally true?"; valuable but convergent/analytical rather than generative |
| **The Futurist** | Analytical | Evaluative, Convergent | Yellow Hat (opportunities), CPS (explore vision), White Hat (trends/data) | Specificity, Analysis-invitation | **Medium** — Generates trend-based questions, but these tend toward analytical extrapolation rather than imaginative futures; "What trends suggest...?" rather than "What if the future were...?" |
| **The Lawyer** | Specialized Lens | Convergent, Evaluative | Black Hat (risks/compliance), White Hat (facts/regulations) | Specificity, Analysis-invitation, Constraint (narrow) | **Low** — Generates compliance-focused questions; highly specialized and convergent; better at answering regulatory questions than generating exploratory ones |
| **The Momentum Builder** | Generative | Generative, Divergent | Green Hat (creative), Yellow Hat (benefits), CPS (ideate, implement) | Open-endedness, Positive framing, Imagination-invitation | **High** — Generates progressive questions like "What could we build on this?" and "How might we take this further?"; naturally positive and forward-building |
| **The Politician** | Specialized Lens | Evaluative, Convergent | Black Hat (risks), White Hat (stakeholder data), Blue Hat (process) | Specificity, Analysis-invitation | **Low** — Generates stakeholder-focused questions, but these are evaluative and political rather than generative; better at answering "Who needs to be involved?" than opening new territory |
| **The Pragmatist** | Analytical | Convergent, Evaluative | Black Hat (feasibility), CPS (acceptance-finding, develop), White Hat (facts) | Constraint (narrow), Analysis-invitation, Specificity | **Low** — Generates feasibility questions like "Can we actually do this?" and "What's the minimum viable approach?"; important for evaluation but not for opening question territory |
| **The Provocateur** | Generative | Divergent, Provocative | Green Hat (creative), Lateral Thinking (provocation/PO, challenge), CPS (ideate) | Freedom from assumptions, Imagination-invitation, Open-endedness | **Very High** — Naturally generates absurdist, provocative questions like "What if we did the opposite?" and "PO: What if failure were impossible?"; excellent at breaking conventional thought patterns |
| **The Questioner** | Analytical | Convergent, Socratic (all categories) | Socratic Method, Blue Hat (meta-questions), White Hat (clarification) | Assumptions (probing), Analysis-invitation, Constraint (narrow) | **Low** — Ironically, despite the name, generates meta-questions about questions rather than generative questions about topics; retired from Phase 3 for lack of distinctiveness; better at probing than opening |
| **The Simplifier** | Generative | Convergent (subtractive), Socratic (questioning assumptions) | Lateral Thinking (challenge), CPS (formulate challenges), Green Hat (alternatives) | Freedom from assumptions, Constraint (focused), Analysis-invitation | **Medium** — Generates subtractive questions like "What could we remove?" and "What's the simplest version?"; valuable but convergent rather than divergent |
| **The Storyteller** | Human-Centered | Generative, Divergent | Red Hat (emotional resonance), Appreciative Inquiry (narrative focus), Design Thinking (HMW) | Open-endedness, Imagination-invitation, Positive framing | **High** — Generates narrative questions like "What's the user's journey?" and "What story does this tell?"; naturally invites imaginative, scenario-based thinking |
| **The Synthesizer** | Connective | Convergent, Meta-level | Blue Hat (process/summary), CPS (develop), Socratic (meta-questions) | Constraint (focused), Analysis-invitation | **Low** — Better at synthesizing existing questions/ideas than generating new ones; naturally convergent rather than divergent |
| **The Systems Thinker** | External Perspectives | Divergent, Socratic (probing implications) | White Hat (data/patterns), Green Hat (alternatives), CPS (formulate challenges) | Open-endedness, Assumptions (challenging), Imagination-invitation | **High** — Generates systems-oriented questions like "What are the second-order effects?" and "What feedback loops exist?"; opens systems-level question territory |
| **The Technical Expert** | Analytical | Convergent, Evaluative | White Hat (technical facts), Black Hat (technical feasibility), CPS (develop) | Specificity, Analysis-invitation, Constraint (narrow) | **Low** — Generates technical feasibility questions; highly specialized and convergent; better at answering technical questions than generating exploratory ones |
| **The Visionary** | Generative | Divergent, Generative, Provocative | Green Hat (creative), Yellow Hat (opportunities), Appreciative Inquiry (dream), Lateral Thinking (challenge) | Freedom from assumptions, Imagination-invitation, Open-endedness, Positive framing | **Very High** — Naturally generates transformative questions like "What if we reimagined everything?" and "What paradigm shift is possible?"; excellent at opening imaginative, aspirational question territory |

### 2.2 Coverage Summary

#### Well-Covered Question Types/Modes

**Analytical/Convergent questions:**
- Socratic questioning (clarification, probing assumptions, probing reasons/evidence, probing implications, meta-questions)
- White Hat (facts/information)
- Black Hat (caution/critical judgment)
- CPS fact-finding and problem-finding
- Evaluative framing

**Personas providing coverage:** Analyst, Questioner, First Principles Thinker, Devil's Advocate, Pragmatist, Technical Expert, Accountant, Lawyer, Futurist

**Assessment:** **Oversaturated** — 9 of 22 personas (41%) are primarily analytical/convergent, with strong overlap in the types of questions they naturally generate.

#### Moderately Covered Question Types/Modes

**Connective/Cross-domain questions:**
- Socratic questioning (questioning viewpoints)
- Lateral Thinking (random entry)
- Cross-domain pattern recognition

**Personas providing coverage:** Analogist, Connector, Systems Thinker

**Assessment:** **Adequate** — 3 personas provide distinct connective question modes (cross-domain importing, structural parallels, systems dynamics).

**Human-centered/Empathic questions:**
- Red Hat (feelings/intuition)
- Design Thinking (HMW)
- User-focused generative questions

**Personas providing coverage:** Audience Advocate, Empath, Storyteller

**Assessment:** **Adequate** — 3 personas provide distinct human-centered question modes (user needs, emotional resonance, narrative journeys).

#### Systematically Absent Question Types/Modes

**Appreciative Inquiry questions (positive/affirmative framing):**
- Discover: "Tell me about a time when X was at its absolute best..."
- Dream: "Imagine X three years from now, operating at its highest potential..."
- Design: "What provocative propositions can we create..."
- Destiny: "What immediate actions can each of us take..."

**Personas with natural alignment:** Empath (partial), Visionary (partial)

**Assessment:** **Critical gap** — No persona is explicitly designed to generate Appreciative Inquiry questions. While Empath and Visionary have some alignment, neither naturally generates the "tell me about a time when..." discovery questions or the "provocative propositions" design questions that characterize AI methodology. The roster lacks a persona that consistently frames questions in affirmative, strength-based terms.

**Green Hat questions (pure creative possibility):**
- "What entirely new ways could we imagine...?"
- "If there were no constraints, how would we reinvent...?"
- "What alternative approaches could spark breakthrough ideas?"

**Personas with partial alignment:** Provocateur, Visionary, Constraint Flipper, Momentum Builder

**Assessment:** **Moderate gap** — While several generative personas exist, none is explicitly designed to generate pure Green Hat questions focused on creative possibility without provocation, constraint-flipping, or paradigm-shifting. The Provocateur uses absurdism, the Visionary uses paradigm shifts, the Constraint Flipper uses reframing — but there's no persona that simply asks "What are all the creative possibilities?"

**Lateral Thinking Provocation (PO) questions:**
- "PO: All team meetings must be conducted in silence."
- "PO: Every team member must disagree with the last idea proposed."

**Personas with natural alignment:** Provocateur

**Assessment:** **Single-persona coverage** — Only the Provocateur naturally generates provocative/absurdist questions. This is a high-value but narrow coverage.

**Yellow Hat questions (optimism/benefits):**
- "What are the potential benefits of...?"
- "How could X positively impact...?"
- "What opportunities might arise if...?"

**Personas with partial alignment:** Momentum Builder, Visionary

**Assessment:** **Moderate gap** — While Momentum Builder and Visionary have positive orientations, neither is explicitly designed to generate benefit-focused, opportunity-identifying questions. The roster lacks a persona that consistently asks "What's the upside?" or "What opportunities does this create?"

### 2.3 Question-Generation Fitness Rankings

#### Very High Fitness (Naturally suited to generating diverse, high-quality questions)

1. **The Empath** — Only persona naturally aligned with Appreciative Inquiry; generates emotionally resonant, affirmative questions
2. **The Provocateur** — Generates absurdist, provocative questions that break conventional thought patterns
3. **The Visionary** — Generates transformative, paradigm-shifting, aspirational questions

#### High Fitness (Strong question-generation capabilities)

4. **The Analogist** — Generates cross-domain questions that open new territories
5. **The Audience Advocate** — Generates empathic, user-focused HMW questions
6. **The Connector** — Generates structural parallel questions
7. **The Constraint Flipper** — Generates reframing questions that challenge embedded assumptions
8. **The Momentum Builder** — Generates progressive, forward-building questions
9. **The Storyteller** — Generates narrative, scenario-based questions
10. **The Systems Thinker** — Generates systems-oriented, second-order questions

#### Medium Fitness (Useful but limited question-generation capabilities)

11. **The Devil's Advocate** — Generates important risk-probing questions, but evaluative/convergent
12. **The First Principles Thinker** — Generates foundational questions, but convergent/analytical
13. **The Accountant** — Generates financial questions, but evaluative/specialized
14. **The Futurist** — Generates trend-based questions, but analytical extrapolation
15. **The Simplifier** — Generates subtractive questions, but convergent

#### Low Fitness (Better suited to answering questions than generating them)

16. **The Analyst** — Generates convergent, evaluative questions; lacks distinctiveness
17. **The Questioner** — Generates meta-questions about questions; lacks generative power
18. **The Pragmatist** — Generates feasibility questions; evaluative rather than generative
19. **The Technical Expert** — Generates technical feasibility questions; highly specialized
20. **The Lawyer** — Generates compliance-focused questions; highly specialized
21. **The Politician** — Generates stakeholder-focused questions; evaluative/political
22. **The Synthesizer** — Better at synthesizing existing questions than generating new ones

**Critical insight:** The current Phase 2B persona set draws heavily from the **Medium and Low fitness categories**, while excluding most of the **Very High and High fitness personas**.

---

## 3. Current Phase 2B Set Assessment

### 3.1 Current Phase 2B Composition

**Medium effort (3 personas):** The Questioner, The Analyst, The Audience Advocate

**High effort (5 personas):** + The Devil's Advocate, The First Principles Thinker

### 3.2 Coverage Gaps

The current Phase 2B set exhibits **systematic gaps** across multiple question taxonomy categories and framework modes:

#### Question Taxonomy Gaps

| Taxonomy Category | Present in Phase 2B? | Missing Question Types |
|:------------------|:---------------------|:-----------------------|
| Socratic: Clarification | ✓ (Questioner, Analyst, Audience Advocate) | — |
| Socratic: Probing Assumptions | ✓ (Questioner, Analyst, First Principles Thinker, Devil's Advocate) | — |
| Socratic: Probing Reasons/Evidence | ✓ (Questioner, Analyst) | — |
| Socratic: Questioning Viewpoints | Partial (Audience Advocate) | Cross-domain viewpoints, structural parallels |
| Socratic: Probing Implications | ✓ (Devil's Advocate, First Principles Thinker) | — |
| Socratic: Meta-questions | ✓ (Questioner) | — |
| **Divergent Questions** | **✗** | **All divergent question types** |
| **Convergent Questions** | ✓ (All five personas) | — |
| **Generative Framing** | Partial (Audience Advocate) | Appreciative, imaginative, aspirational generative questions |
| **Evaluative Framing** | ✓ (All five personas) | — |
| Surface-Level Questions | ✓ (Analyst, Questioner) | — |
| Deep-Level Questions | ✓ (First Principles Thinker, Devil's Advocate) | — |

**Summary:** Phase 2B has **complete coverage of convergent, evaluative, and Socratic analytical questions**, but **zero coverage of divergent questions** and **minimal coverage of generative framing**.

#### Framework Gaps

| Framework Mode | Present in Phase 2B? | Missing Question Modes |
|:---------------|:---------------------|:-----------------------|
| **Appreciative Inquiry (all phases)** | **✗** | **Discover, Dream, Design, Destiny questions** |
| CPS: Objective Finding | Partial (First Principles Thinker) | Vision-oriented objective finding |
| CPS: Fact-Finding | ✓ (Analyst, Questioner) | — |
| CPS: Problem-Finding | ✓ (Analyst, First Principles Thinker) | — |
| CPS: Solution-Finding (Ideate) | Partial (Audience Advocate) | Broad solution-finding questions |
| CPS: Solution-Finding (Develop/Evaluate) | ✓ (Devil's Advocate, First Principles Thinker) | — |
| CPS: Acceptance-Finding | ✗ | Implementation-focused questions |
| Design Thinking: HMW | ✓ (Audience Advocate) | — |
| **Lateral Thinking: Random Entry** | **✗** | **All random entry questions** |
| **Lateral Thinking: Provocation (PO)** | **✗** | **All provocative questions** |
| **Lateral Thinking: Movement** | **✗** | **All movement questions** |
| Lateral Thinking: Challenge | Partial (Devil's Advocate, First Principles Thinker) | Creative challenge questions |
| Six Hats: White Hat | ✓ (Analyst, Questioner) | — |
| **Six Hats: Red Hat** | **✗** | **All emotional/intuitive questions** |
| Six Hats: Black Hat | ✓ (Devil's Advocate) | — |
| **Six Hats: Yellow Hat** | **✗** | **All optimism/benefit questions** |
| **Six Hats: Green Hat** | **✗** | **All creative possibility questions** |
| Six Hats: Blue Hat | Partial (Questioner) | Process control questions |

**Summary:** Phase 2B has **zero coverage** of:
- All Appreciative Inquiry phases
- Lateral Thinking (Random Entry, Provocation, Movement)
- Red Hat (emotional/intuitive questions)
- Yellow Hat (optimism/benefit questions)
- Green Hat (creative possibility questions)

### 3.3 Dimension Skew

Analyzing Phase 2B against the Question Quality Dimensions rubric:

| Dimension | Phase 2B Emphasis | Overrepresented | Underrepresented |
|:----------|:------------------|:----------------|:-----------------|
| **Open-endedness** | Low-Medium | Closed/convergent questions | Open-ended divergent questions |
| **Constraint** | Narrow | Narrow, focused questions | Broad, exploratory questions |
| **Specificity** | High | Specific, targeted questions | Broad, possibility-opening questions |
| **Assumptions** | Probing | Assumption-probing questions | Assumption-challenging questions (reframing) |
| **Invitation** | Analysis | Analytical questions | Imaginative questions |
| **Positive Framing** | Low | Problem-focused, risk-focused questions | Strength-based, opportunity-focused questions |

**Summary:** Phase 2B is **heavily skewed toward analytical, convergent, narrow, assumption-probing questions** while **systematically underrepresenting imaginative, divergent, broad, assumption-challenging, and positively-framed questions**.

### 3.4 Bias Risk Assessment

Based on the bias research findings, the current all-analytical Phase 2B set introduces **multiple systematic biases**:

#### Anchoring Bias

**Risk:** The first questions generated in Phase 2B will anchor all subsequent thinking. If all Phase 2B personas generate analytical, convergent, problem-focused questions, these will anchor the entire brainstorming session toward analytical territory.

**Specific concern:** Because Phase 2B questions are synthesized into topic clusters that define the territory for **all Phase 3 personas**, analytical anchoring in Phase 2B constrains even the generative Phase 3 personas (Provocateur, Visionary, Constraint Flipper) to operate within analytically-defined territory.

**Example scenario:**
- Phase 2B generates questions like: "What are the risks of X?", "What assumptions underlie Y?", "What evidence supports Z?"
- Phase 2B synthesis produces clusters like: "Risk Assessment", "Assumption Validation", "Evidence Requirements"
- Phase 3 Provocateur is now constrained to generate provocative ideas *within* risk assessment territory, rather than opening entirely new territory like "Celebration Opportunities" or "Transformative Possibilities"

#### Framing Bias

**Risk:** Analytical framing suppresses creative responses. The research explicitly documents that when innovators are compelled to operate within adaptive/analytical frameworks, their perceived creativity decreases.

**Specific concern:** All five Phase 2B personas use analytical framing:
- **Questioner:** Socratic analytical framing
- **Analyst:** Systematic breakdown framing
- **Audience Advocate:** User-needs framing (generative, but still analytical in approach)
- **Devil's Advocate:** Risk/critique framing
- **First Principles Thinker:** Deconstruction framing

**Zero personas use:**
- Appreciative framing ("Tell me about a time when...")
- Provocative framing ("PO: What if...")
- Imaginative framing ("What if there were no constraints...")
- Affirmative framing ("What's working well that we could amplify?")

#### Question-Response Loop Bias

**Risk:** The type of questions asked predetermines the type of answers generated. Analytical questions → analytical responses; creative questions → creative responses.

**Specific concern:** In the isolated subagent architecture, each Phase 2B persona independently generates 15-20 questions. If all personas generate analytical questions, the synthesis step receives 45-100 analytical questions (Medium effort: 3×15-20; High effort: 5×15-20) and **zero generative/imaginative/affirmative questions**.

The synthesis step can only work with the questions it receives. Even if the synthesizer attempts to "balance" the clusters, it cannot introduce question types that were never generated. The result is analytically-bounded topic clusters that propagate analytical bias into Phase 3.

#### Cognitive Diversity Bias

**Risk:** Homogeneous cognitive styles among question-generators produce inferior ideation outcomes compared to diverse teams.

**Specific concern:** All five Phase 2B personas are classified as **Analytical** in the persona taxonomy. Zero Generative personas. Zero Connective personas. This is the definition of cognitive homogeneity.

The research explicitly states: "When innovators are compelled to operate within a purely adaptive/analytical framework, their perceived creativity decreases." The Phase 2B set is a purely adaptive/analytical framework.

#### Cumulative Bias Impact

The biases compound:

1. **Anchoring** → First analytical questions anchor thinking to analytical territory
2. **Framing** → Analytical framing suppresses creative responses
3. **Question-Response Loop** → Analytical questions → analytical topic clusters → analytical brainstorming territory
4. **Cognitive Homogeneity** → No generative/creative questions to counterbalance analytical bias

**Net result:** Phase 2B systematically constrains the entire brainstorming session to analytical territory, regardless of the cognitive diversity present in Phase 3 personas.

### 3.5 Alternative Compositions

Based on the coverage analysis and bias assessment, the following alternative compositions would provide superior question diversity:

#### Alternative 1: Balanced Cognitive Diversity (Recommended)

**3-persona set (Medium effort):**
1. **The Audience Advocate** (Human-Centered) — HMW questions, user-focused generative questions
2. **The Empath** (Human-Centered/Generative) — Appreciative Inquiry questions, emotionally resonant questions, Red Hat questions
3. **The Provocateur** (Generative) — Lateral Thinking provocation questions, Green Hat questions, assumption-challenging questions

**5-persona set (High effort):**
1. **The Audience Advocate** (Human-Centered) — HMW questions, user-focused generative questions
2. **The Empath** (Human-Centered/Generative) — Appreciative Inquiry questions, Red Hat questions
3. **The Provocateur** (Generative) — Lateral Thinking provocation, Green Hat questions
4. **The Systems Thinker** (External Perspectives) — Systems-oriented questions, second-order implications
5. **The First Principles Thinker** (Analytical) — Foundational assumption-probing questions

**Rationale:**
- **Eliminates analytical monoculture:** Replaces 3-5 analytical personas with 2-4 generative/human-centered personas + 0-1 analytical persona
- **Covers critical gaps:** Adds Appreciative Inquiry (Empath), Lateral Thinking Provocation (Provocateur), Red/Green Hats (Empath/Provocateur), systems thinking (Systems Thinker)
- **Maintains analytical rigor:** Retains First Principles Thinker in 5-persona set to ensure foundational questions are still asked
- **Question-generation fitness:** All personas are High or Very High fitness for question generation (vs. current set with 2 Low fitness personas: Analyst, Questioner)

**Expected impact:**
- Phase 2B generates balanced mix of analytical, generative, empathic, provocative, and systems-oriented questions
- Topic clusters span analytical territory (risks, assumptions, evidence) AND generative territory (possibilities, transformations, emotional resonance, systemic opportunities)
- Phase 3 personas operate in expanded territory, unconstrained by analytical anchoring

#### Alternative 2: Maximum Divergence (For highly creative/exploratory topics)

**3-persona set:**
1. **The Visionary** (Generative) — Paradigm-shifting, aspirational questions
2. **The Provocateur** (Generative) — Absurdist, provocative questions
3. **The Analogist** (Connective) — Cross-domain questions

**5-persona set:**
1. **The Visionary** (Generative) — Paradigm-shifting questions
2. **The Provocateur** (Generative) — Provocative questions
3. **The Analogist** (Connective) — Cross-domain questions
4. **The Empath** (Human-Centered/Generative) — Appreciative Inquiry, emotional questions
5. **The Constraint Flipper** (Generative) — Reframing questions

**Rationale:**
- **Maximizes divergent thinking:** All personas are High or Very High fitness for divergent question generation
- **Zero analytical personas:** Deliberately excludes analytical personas to counteract typical analytical bias in business/organizational contexts
- **Best for:** Early-stage ideation, paradigm-shifting initiatives, creative challenges, topics where analytical thinking is already oversaturated

**Trade-off:** Sacrifices analytical rigor for maximum creative exploration; may need analytical follow-up in later phases

#### Alternative 3: Framework-Balanced (Covers all major frameworks)

**5-persona set:**
1. **The Empath** (Human-Centered/Generative) — Appreciative Inquiry, Red Hat
2. **The Provocateur** (Generative) — Lateral Thinking, Green Hat
3. **The Devil's Advocate** (Analytical) — Black Hat, CPS evaluation
4. **The Audience Advocate** (Human-Centered) — Design Thinking HMW, user-focused
5. **The Systems Thinker** (External Perspectives) — Systems thinking, implications

**Rationale:**
- **Framework coverage:** Explicitly covers Appreciative Inquiry, Lateral Thinking, Six Thinking Hats (Red/Green/Black), Design Thinking, Systems Thinking
- **Balanced cognitive modes:** 2 Generative, 2 Human-Centered, 1 Analytical, 1 External Perspectives
- **Maintains some analytical presence:** Devil's Advocate ensures risk/critique questions are asked

**Best for:** General-purpose brainstorming where comprehensive framework coverage is desired

#### Alternative 4: Topic-Aware Selection (Adaptive approach)

**Concept:** Instead of fixed persona assignments, select Phase 2B personas based on topic characteristics.

**Selection criteria:**

| Topic Characteristic | Recommended Phase 2B Personas |
|:---------------------|:------------------------------|
| **Highly analytical/technical** | Add generative personas to counterbalance: Provocateur, Visionary, Empath |
| **Highly creative/artistic** | Add analytical personas to ground: First Principles Thinker, Systems Thinker |
| **User-focused/service design** | Audience Advocate, Empath, Storyteller |
| **Financial/business model** | Accountant, Pragmatist, Visionary (to balance analytical with aspirational) |
| **Organizational/cultural** | Empath, Politician, Systems Thinker |
| **Exploratory/paradigm-shifting** | Visionary, Provocateur, Analogist, Constraint Flipper |
| **Risk-heavy/compliance** | Devil's Advocate, Lawyer, First Principles Thinker |

**Rationale:**
- **Adaptive to context:** Recognizes that different topics benefit from different question-generation approaches
- **Counterbalances inherent topic bias:** Adds generative personas to analytical topics and vice versa
- **Maximizes relevance:** Ensures question-generators are well-suited to the topic domain

**Trade-off:** Requires topic classification logic; more complex than fixed assignments

---

## 4. Actionable Implications

### 4.1 For Phase 2B Persona Selection

**Primary recommendation: Adopt Alternative 1 (Balanced Cognitive Diversity) as the new default Phase 2B set.**

**Rationale:**

1. **Eliminates analytical monoculture:** The current all-analytical set introduces systematic bias that constrains downstream brainstorming. Alternative 1 provides balanced cognitive diversity.

2. **Covers critical gaps:** Alternative 1 adds coverage for Appreciative Inquiry, Lateral Thinking Provocation, Red/Green Hats, and systems thinking — all systematically absent from the current set.

3. **Improves question-generation fitness:** Alternative 1 replaces Low fitness personas (Analyst, Questioner) with Very High fitness personas (Empath, Provocateur), improving overall question quality.

4. **Maintains analytical rigor where needed:** The 5-persona version retains First Principles Thinker to ensure foundational questions are still asked, while the 3-persona version relies on Audience Advocate's analytical user-needs focus.

5. **Aligns with research consensus:** The cognitive diversity research explicitly states that diverse cognitive styles among question-generators produce better ideation outcomes than homogeneous groups. Alternative 1 operationalizes this finding.

**Secondary recommendation: Consider topic-aware selection (Alternative 4) for advanced implementation.**

For topics that are inherently analytical (technical, financial, compliance-heavy), the default balanced set may still be too analytical. For topics that are inherently creative (artistic, exploratory, paradigm-shifting), the balanced set may be too conservative. Topic-aware selection allows the system to adapt to context.

**Implementation approach:**

1. **Phase 1:** Replace current fixed Phase 2B set with Alternative 1 (Balanced Cognitive Diversity)
2. **Phase 2:** Gather data on question quality and downstream brainstorming outcomes
3. **Phase 3:** If data supports, implement topic-aware selection with Alternative 1 as default fallback

**What NOT to do:**

- **Do not retain the current all-analytical set.** The research unambiguously demonstrates that cognitive homogeneity among question-generators produces inferior outcomes.
- **Do not add a single generative persona to the current analytical set.** Adding one Provocateur to Questioner/Analyst/Audience Advocate/Devil's Advocate/First Principles Thinker would still leave the set 80% analytical, insufficient to counteract analytical anchoring and framing bias.
- **Do not assume the synthesis step can compensate for input bias.** The synthesis step can only work with the questions it receives; it cannot introduce question types that were never generated.

### 4.2 For Question Synthesis (Phase 2B.2)

**Current challenge:** Even with improved Phase 2B persona selection, the synthesis step must consolidate 45-100 individual questions into 3-5 coherent topic clusters. This consolidation process can inadvertently reintroduce bias if not carefully managed.

**Recommendations for synthesis step:**

#### 1. Preserve Question Type Diversity in Clusters

**Current risk:** Synthesis may inadvertently "normalize" questions toward analytical framing. For example:
- Input: "Tell me about a time when team collaboration was at its absolute best." (Appreciative Inquiry, Empath)
- Normalized: "What factors contribute to effective team collaboration?" (Analytical reframing)

**Countermeasure:** Synthesis prompt should explicitly instruct the synthesizer to **preserve the framing and cognitive mode of the original questions** rather than normalizing them.

**Synthesis instruction addition:**
> "When consolidating questions into topic clusters, preserve the diversity of question types and framing modes. Do not normalize Appreciative Inquiry questions into analytical questions, provocative questions into conventional questions, or imaginative questions into analytical questions. Each cluster should contain a mix of question types (analytical, generative, empathic, provocative, systems-oriented) that reflect the diversity of the input questions."

#### 2. Explicitly Monitor for Analytical Bias

**Current risk:** Without explicit guidance, the synthesizer may unconsciously favor analytical questions as "more serious" or "more rigorous."

**Countermeasure:** Add bias-checking step to synthesis process.

**Synthesis instruction addition:**
> "After creating topic clusters, review them for cognitive diversity. Each cluster should include:
> - At least one generative/imaginative question (What if...? How might we...? What possibilities...?)
> - At least one analytical question (What are the risks? What assumptions? What evidence?)
> - At least one human-centered question (How would users feel? What do people need? What's the user journey?)
> 
> If a cluster lacks diversity, revise it to include missing question types from the input questions."

#### 3. Create "Aspirational" or "Transformative" Clusters

**Current risk:** Topic clusters may default to problem-focused framing ("Challenges", "Risks", "Constraints") rather than opportunity-focused framing ("Possibilities", "Transformations", "Aspirations").

**Countermeasure:** Explicitly instruct synthesizer to create at least one aspirational/transformative cluster.

**Synthesis instruction addition:**
> "Ensure that at least one topic cluster is framed aspirationally or transformatively, focusing on possibilities, opportunities, transformations, or ideal futures rather than problems, risks, or constraints. Examples: 'Transformative Possibilities', 'Aspirational Futures', 'Opportunity Space', 'What Success Looks Like'."

#### 4. Use Framework Language in Cluster Titles

**Current risk:** Cluster titles may default to generic analytical language ("Analysis", "Evaluation", "Assessment") rather than framework-specific language that signals cognitive mode.

**Countermeasure:** Use framework language in cluster titles to signal the cognitive mode.

**Examples:**
- Instead of "Risk Analysis" → "Black Hat: Risks and Cautions"
- Instead of "User Needs" → "HMW: User Experience Opportunities"
- Instead of "Future Scenarios" → "Appreciative Dream: Ideal Futures"
- Instead of "Challenges" → "Provocations: What If We Flipped Everything?"

This makes the cognitive mode explicit and signals to Phase 3 personas what type of thinking each cluster invites.

### 4.3 For Prompt Design

**Finding:** Question framing significantly influences response quality. The research on question quality dimensions, bias, and cognitive diversity has direct implications for how Phase 2B persona prompts should be designed.

**Recommendations for Phase 2B persona prompts:**

#### 1. Explicitly Instruct Personas on Question Type

**Current approach:** Persona prompts describe the persona's cognitive mode and characteristics but do not explicitly instruct them on what types of questions to generate.

**Recommended addition:** Add explicit question-type guidance to each Phase 2B persona prompt.

**Example for The Empath:**
> "Your role in Phase 2 is to generate questions that explore the emotional and human dimensions of the topic. Focus on:
> - **Appreciative Inquiry questions:** 'Tell me about a time when...' (Discover), 'Imagine X at its highest potential...' (Dream), 'What would bring...' (Design)
> - **Red Hat questions:** 'How would people feel about...?', 'What's your gut reaction to...?', 'What emotional needs...'
> - **Human-centered questions:** 'What would make people's lives better?', 'What brings joy/meaning/connection?', 'What do people truly care about?'
> 
> Avoid analytical or evaluative questions. Your questions should invite emotional resonance and affirmative imagination, not analytical breakdown."

**Example for The Provocateur:**
> "Your role in Phase 2 is to generate provocative questions that challenge conventional thinking. Focus on:
> - **Lateral Thinking Provocation (PO):** 'PO: What if we did the exact opposite?', 'PO: What if failure were impossible?'
> - **Green Hat questions:** 'What entirely new approaches could we imagine?', 'If there were no constraints, how would we reinvent this?'
> - **Challenge questions:** 'Why do we assume X?', 'What if the conventional wisdom is wrong?'
> 
> Your questions should be deliberately provocative, absurdist, or unconventional. Avoid reasonable, practical, or conventional questions."

#### 2. Provide Question Examples in Prompts

**Current approach:** Persona prompts include examples of persona *responses* to brainstorming topics, but not examples of *questions* the persona would generate.

**Recommended addition:** Add 5-10 example questions for each Phase 2B persona to illustrate the desired question type.

**Rationale:** Examples provide concrete anchors for the desired question style, reducing ambiguity and improving consistency.

#### 3. Warn Against Analytical Normalization

**Current risk:** Even generative personas may default to analytical question framing if not explicitly warned against it.

**Recommended addition for generative personas:**
> "Avoid framing your questions analytically or evaluatively. Do not ask 'What are the risks of X?' or 'Is Y feasible?' — those are analytical questions. Instead, ask 'What possibilities does X open?' or 'How might we make Y possible?' — generative questions that invite imagination rather than analysis."

#### 4. Specify Question Quantity and Clustering

**Current approach:** Phase 2B personas are instructed to generate "15-20 questions organized into 3-5 thematic clusters."

**Recommended refinement:** Specify that questions within each persona's clusters should maintain the persona's cognitive mode.

**Addition:**
> "Organize your 15-20 questions into 3-5 thematic clusters. Each cluster should maintain your cognitive mode — do not mix analytical questions with generative questions, or provocative questions with conventional questions. Your clusters should be coherent in both topic and question type."

**Rationale:** This ensures that the synthesis step receives clearly differentiated question clusters, making it easier to preserve cognitive diversity in the final consolidated clusters.

#### 5. Add Quality Dimension Guidance

**Recommended addition based on Question Quality Dimensions rubric:**

For **generative personas** (Empath, Provocateur, Visionary):
> "Your questions should be:
> - **Open-ended:** Invite wide range of responses, not yes/no
> - **Broad enough for creativity:** Avoid being too narrow or solution-prescriptive
> - **Free from embedded assumptions:** Challenge existing assumptions rather than reinforcing them
> - **Imagination-inviting:** Ask 'What if...?' and 'How might we...?' rather than 'What is...?' and 'How do we...?'
> - **Positively framed:** Focus on desired outcomes, possibilities, and opportunities"

For **analytical personas** (First Principles Thinker, if retained):
> "Your questions should be:
> - **Assumption-probing:** Challenge what's taken for granted
> - **Deep rather than surface-level:** Ask 'Why?' and 'What's fundamentally true?' not just 'What?' and 'When?'
> - **Focused but not solution-prescriptive:** Narrow enough to be clear, broad enough to allow multiple approaches"

---

## Appendix: Framework Source Summary

### Question Taxonomies

1. **Socratic Questioning**
   - Source: University of Michigan, "6 types of Socratic Questions"
   - URL: https://websites.umich.edu/~elements/probsolv/strategy/cthinking.htm
   - Coverage: Comprehensive framework for analytical questioning

2. **Divergent vs. Convergent Thinking**
   - Source: Asana, "Convergent vs. Divergent Thinking: Finding Balance [2026]"
   - URL: https://asana.com/resources/convergent-vs-divergent
   - Coverage: Practitioner guidance on balancing question modes

3. **Generative vs. Evaluative Research**
   - Source: Parallel HQ, "Generative vs Evaluative Research: Guide (2025)"
   - URL: https://www.parallelhq.com/blog/generative-vs-evaluative-research
   - Coverage: Research methodology distinguishing question framing modes

4. **Question Depth**
   - Source: Ten Directions, "Unlocking Depth in Conversations: A Hack for Inspired Leadership"
   - URL: https://tendirections.com/unlocking-depth-in-conversations-a-hack-for-inspired-leadership/
   - Coverage: Practitioner guidance on surface vs. deep questions

### Facilitation Frameworks

5. **Appreciative Inquiry**
   - Source: FSG, "Guide to Appreciative Inquiry"
   - URL: https://www.fsg.org/wp-content/uploads/2021/08/Guide-to-Appreciative-Inquiry.pdf
   - Coverage: Comprehensive guide to positive/affirmative question framing

6. **Creative Problem Solving (CPS)**
   - Source: Creative Education Foundation, "What is CPS?"
   - URL: https://www.creativeeducationfoundation.org/what-is-cps/
   - Coverage: Structured methodology for problem-finding vs. solution-finding questions

7. **Design Thinking (How Might We)**
   - Source: Nielsen Norman Group, "Using 'How Might We' Questions to Ideate on the Right Problems"
   - URL: https://www.nngroup.com/articles/how-might-we-questions/
   - Coverage: Detailed guidance on HMW question formulation and scoping

8. **Lateral Thinking**
   - Source: BetterUp, "What is Lateral Thinking? Definition & 7 Techniques to Do It Right"
   - URL: https://www.betterup.com/blog/what-is-lateral-thinking
   - Coverage: Techniques for random entry, provocation, movement, challenge

9. **Six Thinking Hats**
   - Source: EncounterEDU, "Use the six thinking hats to generate questions"
   - URL: https://encounteredu.com/cpd/subject-updates/how-to-use-the-six-thinking-hats-to-generate-questions
   - Coverage: Application of Six Hats framework to question generation

### Cognitive Diversity

10. **Cognitive Diversity in Teams**
    - Source: The Mental Game Clinic, "Cognitive Diversity: How Varied Thinking Styles Drive Team Innovation and Success"
    - URL: https://thementalgame.me/blog/cognitive-diversity-how-different-thinking-styles-boost-team-performance
    - Coverage: Impact of diverse cognitive styles on team performance

11. **Cognitive Diversity in Asset Management**
    - Source: Edmans, A. (2025), Diversity Project
    - URL: https://diversityproject.com/wp-content/uploads/2025/06/DP-Cognitive-Diversity-Full-Research-Paper.pdf
    - Coverage: Research on cognitive diversity's impact on decision-making

12. **Problem Framing and Cognitive Style**
    - Source: Silk, E. M., et al. (2021), Design Studies
    - URL: https://www.sciencedirect.com/science/article/pii/S0142694X21000260
    - Coverage: Empirical research on how cognitive style affects problem framing and ideation

13. **ReQUESTA Framework**
    - Source: Tian, Y., et al. (2026), arXiv preprint
    - URL: https://www.arxiv.org/pdf/2602.03704
    - Coverage: Multi-agent framework for cognitively diverse question generation

14. **Bloom's Taxonomy**
    - Source: Reading Rockets, "Bloom's Taxonomy Questions"
    - URL: https://www.readingrockets.org/sites/default/files/2023-09/Blooms%20Taxonomy%20questions.pdf
    - Coverage: Hierarchical framework for cognitive complexity in questions

### Question Quality

15. **How Might We Questions (Quality Dimensions)**
    - Source: Nielsen Norman Group
    - URL: https://www.nngroup.com/articles/how-might-we-questions/
    - Coverage: Quality criteria for ideation questions (constraint, specificity, assumptions)

16. **Osborn's Brainstorming Critique**
    - Source: Purdue University, "Quality, Conformity, and Conflict: Questioning the Assumptions of Osborn's Brainstorming Technique"
    - URL: https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1093&context=jps
    - Coverage: Critical analysis of brainstorming assumptions and question quality

17. **Divergent Questions**
    - Source: Byrdseed, "Divergent Questions (How To Ask 'Em)"
    - URL: https://www.byrdseed.com/divergent-questions/
    - Coverage: Practitioner guidance on crafting divergent questions

### Bias in Question Generation

18. **Anchoring and Framing Effects**
    - Source: Tversky, A., & Kahneman, D. (1974), Science
    - URL: https://www.science.org/doi/10.1126/science.185.4157.1124
    - Coverage: Foundational research on cognitive biases in judgment

19. **Bias Mitigation in Brainstorming**
    - Source: McKinsey & Company, "Bias Busters: A better way to brainstorm"
    - URL: https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/bias-busters-a-better-way-to-brainstorm
    - Coverage: Evidence-based countermeasures for brainstorming bias

20. **Machine Learning Bias Classification**
    - Source: van Giffen, B., et al. (2022), Journal of Business Research
    - URL: https://doi.org/10.1016/j.jbusres.2022.01.076
    - Coverage: Classification of biases and mitigation methods (applicable to AI-driven question generation)

---

## Conclusion

This research provides comprehensive evidence that the current Phase 2B persona set introduces systematic analytical bias into question generation, constraining downstream brainstorming territory. The all-analytical composition (Questioner, Analyst, Audience Advocate, Devil's Advocate, First Principles Thinker) lacks coverage of critical question-generation modes including Appreciative Inquiry, Lateral Thinking Provocation, Red/Green/Yellow Hat questions, and divergent/imaginative framing.

The recommended solution — replacing the current set with a balanced cognitive diversity composition (Audience Advocate, Empath, Provocateur, Systems Thinker, First Principles Thinker) — addresses these gaps while maintaining analytical rigor where needed. This change aligns with research consensus that diverse cognitive styles among question-generators produce superior ideation outcomes compared to homogeneous groups.

Implementation of these recommendations will require updates to Phase 2B persona selection logic, synthesis step instructions, and persona prompts, but the expected impact on question quality and downstream brainstorming diversity justifies the investment.
